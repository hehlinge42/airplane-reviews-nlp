{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/rwalk/gsdmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gsdmm.gsdmm import MovieGroupProcess\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import pyLDAvis.gensim\n",
    "import operator\n",
    "import pyLDAvis\n",
    "import gensim\n",
    "\n",
    "from src.meta_data_preprocessor import MetaDataPreprocessor\n",
    "from src.text_preprocessor import TextPreprocessor\n",
    "from src.embeddor import Embeddor\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data/inputs\"\n",
    "DATA1 = \"seatguru_python_scraping.csv\"\n",
    "DATA2 = \"skytrax_scraping_2.csv\"\n",
    "\n",
    "df1 = pd.read_csv(os.path.join(DATA_FOLDER, DATA1), index_col=0)\n",
    "df2 = pd.read_csv(os.path.join(DATA_FOLDER, DATA2), index_col=0)\n",
    "concat_df = pd.concat([df1, df2])\n",
    "concat_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/preprocessed/corpus_ngram_data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_data_preprocessor = MetaDataPreprocessor()\n",
    "df_with_metadata = meta_data_preprocessor.preprocess(concat_df)\n",
    "df_with_metadata.to_pickle(os.path.join(\"..\", \"data\", \"preprocessed\", \"reviews_metadata.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_small = df2.copy().iloc[:5000, :]\n",
    "preprocessor = TextPreprocessor(df2_small, column_to_clean='body')\n",
    "preprocessor.transform(n_grams=False, remove_stopwords=True)\n",
    "corpus = preprocessor.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddor = Embeddor(corpus=corpus)\n",
    "embeddor.transform(vec_method=\"word2vec\", how=\"PCA\", n=3)\n",
    "word2vec_embed = embeddor.description_embedding\n",
    "word2vec_model = embeddor.model\n",
    "word2vec_embed['corpus'] = corpus\n",
    "word2vec_embed['rating'] = df2_small['rating']\n",
    "word2vec_embed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary\n",
    "tokens = data.corpus\n",
    "dictionary = gensim.corpora.Dictionary(tokens)\n",
    "dictionary.filter_extremes(no_below=0.05, no_above=0.9)\n",
    "corpus_lda = [dictionary.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaModel = gensim.models.ldamodel.LdaModel(corpus=corpus_lda,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=42,\n",
    "                                           alpha=0.1,\n",
    "                                           eta=0.1,\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in ldaModel.show_topics(formatted=True, num_topics=5, num_words=10):\n",
    "    print(str(i)+\": \"+ topic+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=ldaModel, corpus=data['corpus'], texts=tokens ,coherence=\"c_v\")\n",
    "print(f'Model coherence: {cm.get_coherence()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topic(review, ldaModel=ldaModel, dictionary=dictionary):\n",
    "    review_lda = dictionary.doc2bow(review) \n",
    "    max_prob = 0\n",
    "    max_topic = None\n",
    "    for topic, prob in ldaModel.get_document_topics(review_lda):\n",
    "        if prob > max_prob:\n",
    "            max_topic = topic\n",
    "            max_prob = prob\n",
    "    \n",
    "    return max_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lda_topic'] = data['corpus'].apply(get_lda_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldaModel, corpus_lda, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interior_design_reviews = data[data['lda_topic']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_good = interior_design_reviews.loc[interior_design_reviews['bin']==2, 'corpus'].tolist()\n",
    "corpus_bad = interior_design_reviews.loc[interior_design_reviews['bin']==0, 'corpus'].tolist()\n",
    "\n",
    "mask_good = np.array(Image.open(\"../images/mask2.PNG\"))\n",
    "mask_bad = np.array(Image.open(\"../images/mask3.PNG\"))\n",
    "\n",
    "def build_wordcloud(corpus, mask, colormap=\"viridis\"):\n",
    "    text = \"\"\n",
    "\n",
    "    for review in corpus:\n",
    "        text += \" \".join(review)\n",
    "    wordcloud = WordCloud(collocations=False, background_color=\"white\", max_words=50, mask=mask, colormap=colormap).generate(text)\n",
    "    \n",
    "    return wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_wc = build_wordcloud(corpus_good, mask_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_wc = build_wordcloud(corpus_bad, mask_bad, colormap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(good_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bad_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_wc.to_file('../images/good_wc.png')\n",
    "bad_wc.to_file('../images/bad_wc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_good = interior_design_reviews.loc[interior_design_reviews['bin']==2, 'corpus'].tolist()\n",
    "corpus_bad = interior_design_reviews.loc[interior_design_reviews['bin']==0, 'corpus'].tolist()\n",
    "\n",
    "n_grams_good = interior_design_reviews.loc[interior_design_reviews['bin']==2, 'n_grams'].tolist()\n",
    "n_grams_bad = interior_design_reviews.loc[interior_design_reviews['bin']==0, 'n_grams'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatenned_corpus_good = [val for sublist in corpus_good for val in sublist]\n",
    "good_c = Counter(flatenned_corpus_good)\n",
    "\n",
    "flatenned_corpus_bad = [val for sublist in corpus_bad for val in sublist]\n",
    "bad_c = Counter(flatenned_corpus_bad)\n",
    "\n",
    "flatenned_n_grams_good = [val for sublist in n_grams_good for val in sublist]\n",
    "good_n = Counter(flatenned_n_grams_good)\n",
    "\n",
    "flatenned_n_grams_bad = [val for sublist in n_grams_bad for val in sublist]\n",
    "bad_n = Counter(flatenned_n_grams_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_n.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_n.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.groupby(by=['year', 'month', 'lda_topic'])['corpus'].count().reset_index()\n",
    "fig, ax = plt.subplots()\n",
    "(data[(data['lda_topic'].isin([0, 1, 2]))&(data['year']>=2014)]\n",
    "     .pivot_table(values='corpus', columns='lda_topic', index=['year', 'month'], aggfunc='count')\n",
    "     .plot(ax=ax))\n",
    "ax.legend([\"Interior Desgin\", \"Positive\", \"Negative\"])\n",
    "plt.title('Topic Evolution over time', fontsize=20)\n",
    "plt.xlabel('Year, Month', fontsize=16)\n",
    "plt.ylabel('Count', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "untokenized_corpus = [\" \".join(words) for words in df_log_reg.corpus]\n",
    "X = vectorizer.fit_transform(untokenized_corpus)\n",
    "model.fit(X, df_log_reg.target.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.LinearExplainer(model, X, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer.shap_values(X)\n",
    "X_array = X.toarray()\n",
    "\n",
    "shap.summary_plot(shap_values, X_array, feature_names=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_small['nb_token'] = list(map(len, df2_small['corpus']))\n",
    "docs = df2_small.corpus.to_list()\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_topic = 10\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "\n",
    "mgpModel = MovieGroupProcess(K=nb_topic, alpha=alpha, beta=beta, n_iters=20)\n",
    "mgpModelFit = mgpModel.fit(tokens, n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topWordsPerTopic(clusterDistrib, topIndex, nbWord):\n",
    "    for index in topIndex:\n",
    "        clusterWord = clusterDistrib[index]\n",
    "        sortedCluster = sorted(clusterWord.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        clusterTopWords = sortedCluster[:nbWord]\n",
    "        print(f\"Cluster {index} : {clusterTopWords}\")\n",
    "        print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docCount = np.array(mgpModel.cluster_doc_count)\n",
    "print('Number of documents per topic :', docCount)\n",
    "print('*'*20)\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "topIndex = docCount.argsort()[::-1]\n",
    "print('Most important clusters (by number of docs inside):', topIndex)\n",
    "print('*'*20)\n",
    "# Show the top 30 words in term frequency for each cluster \n",
    "topWordsPerTopic(mgpModel.cluster_word_distribution, topIndex, 30)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6972ee2bd1b4509a80cf34a71ca27c907c02bd1bee408bb4a0ddede37fe3d1b4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
